{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Migration Patterns using Digital Trace Data  \n",
    "\n",
    "Lilongwe, Malawi, 21 May 2024 from 11:00 to 13:00.  \n",
    "\n",
    "Organized by the [IUSSP Panel in Digital and Computational Demography](https://iussp.org/en/digital-and-computational-demography).\n",
    "\n",
    "This training workshop will take place at the African Population Conference in Lilongwe, Malawi, on 21 May 2024 from 11:00 to 13:00.   \n",
    "**Please register for this training workshop only if you are attending the UAPS conference.**  \n",
    "\n",
    "Register [here](https://docs.google.com/forms/d/e/1FAIpQLSd2hEX9l8FACdzBqtrggkjImEDRz_83ZFnENCpeez_q86mGnw/viewform) for \"*Exploring Migration Patterns using Digital Trace Data*\" \n",
    "\n",
    "## Trainers: \n",
    "•    Carolina Coimbra Vieira, Max Planck Institute for Demographic Research (MPIDR)   \n",
    "•    Ebru Şanlıtürk, Max Planck Institute for Demographic Research (MPIDR) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplcolor\n",
    "import seaborn as sns\n",
    "from mycolorpy import colorlist as mcp\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "    \n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_langs_dict = {\"en\": \"#666666\", \n",
    "                    \"uk\": \"#e7298a\",\n",
    "                    \"pl\": \"#d95f02\",\n",
    "                    \"ru\": \"#1b9e77\",\n",
    "                    \"Ukrainian refugees\": \"#000000\",\n",
    "                    \"DANE\": \"#000000\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries (df, color, title, note):\n",
    "    \n",
    "    df.plot(color=color)\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.savefig(\"figs/series/timeseries-\" + note + \".png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_timeseries (df, color, title, note):\n",
    "    \n",
    "    axes = df.plot.line(subplots=True, color=color, lw=2, ylim=(0,max(df.max())))\n",
    "    axes = axes.flat\n",
    "    fig = axes[0].get_figure()\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.legend(handletextpad=-2.0, handlelength=0, loc='upper left', bbox_to_anchor=(0.08, 1.1), frameon=False)\n",
    "    #plt.xticks(rotation=50)\n",
    "    \n",
    "    plt.savefig(\"figs/series/subplots-views-\" + note + title + \".png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNHCR data\n",
    "The data from UNHCR consists of the daily number of Ukrainian refugees crossing the border from Ukraine to Poland 24th of February 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_official = pd.read_csv(\"data/data_unhcr_border_crossings_from_ukraine_day.csv\", sep=\",\")\n",
    "df_official[\"Date\"] = df_official[\"Date\"].astype('datetime64[ns]')\n",
    "df_official.set_index(\"Date\", inplace=True)\n",
    "\n",
    "df_official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries(df_official, \n",
    "                color=colors_langs_dict, \n",
    "                title=\"Daily number of Ukrainian refugees crossing the border from Ukraine to Poland\", \n",
    "                note=\"UNHCR-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dane = pd.read_csv(\"data/data_dane_applications_cities_pol_week.csv\", index_col=0)\n",
    "df_dane.rename(columns={\"WARSZAWA\": \"WARSAW\"}, inplace=True)\n",
    "df_dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dane_stocks = pd.read_csv(\"data/data_dane_applications_cities_pol_week_stocks.csv\", index_col=0)\n",
    "df_dane_stocks.rename(columns={\"WARSZAWA\": \"WARSAW\"}, inplace=True)\n",
    "df_dane_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_PESEL = pd.DataFrame.from_dict(dict([(c[0].replace(\" DANE\", \"\"), c[1])\n",
    "                                                 for c in sorted(dict(df_dane_stocks.max()).items(), key=lambda x:x[1], reverse=True)]), \n",
    "                                           orient=\"index\")\n",
    "rank_PESEL.columns = [\"Max PESEL stocks\"]\n",
    "rank_PESEL.index = [c.capitalize() for c in rank_PESEL.index]\n",
    "rank_PESEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia data\n",
    "For each Polish city, we collected the data on the number of views on Wikipedia pages using the Wikimedia pageviews platform: https://pageviews.wmcloud.org/langviews/\n",
    "\n",
    "\n",
    "### Biggest Polish cities\n",
    "\n",
    "https://metropolie.pl/fileadmin/news/2022/07/Urban_hospitality_update.pdf\n",
    "https://businessinsider-com-pl.translate.goog/wiadomosci/wiemy-do-ktorych-miast-dotarlo-najwiecej-uchodzcow-z-ukrainy/1cqkwe5?_x_tr_sl=pl&_x_tr_tl=en&_x_tr_hl=pl&_x_tr_pto=wapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country + Polish cities with more than 200k inhabitants\n",
    "polish_cities = [\"Poland\",\n",
    "                 \"Białystok\", \n",
    "                 \"Bydgoszcz\", \n",
    "                 \"Częstochowa\", \n",
    "                 \"Gdańsk\", \n",
    "                 \"Gdynia\", \n",
    "                 \"Gliwice\", \n",
    "                 \"Katowice\", \n",
    "                 \"Kielce\", \n",
    "                 \"Kraków\", \n",
    "                 \"Łódź\", \n",
    "                 \"Lublin\", \n",
    "                 \"Poznań\", \n",
    "                 \"Radom\", \n",
    "                 \"Sosnowiec\", \n",
    "                 \"Szczecin\",\n",
    "                 \"Toruń\",\n",
    "                 \"Warsaw\", \n",
    "                 \"Wrocław\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# use glob to get all the csv files in the folder\n",
    "path = os.getcwd()\n",
    "csv_files_preprocess = glob.glob(os.path.join(path, \"data/data_wikipedia_views_cities_pol/*.csv\"))\n",
    "csv_files_preprocess = list(set(csv_files_preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_files_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = pd.read_csv(\"data/language-codes-full.csv\")\n",
    "language_codes = language_codes[[\"alpha3-b\", \"alpha2\", \"English\"]]\n",
    "\n",
    "language_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_relative_change(csv_files_preprocess, same_scale=False):\n",
    "    colors = [\"#1b9e77\", \"#d95f02\", \"#7570b3\", \"#e7298a\", \"#66a61e\", \"#e6ab02\", \"#a6761d\", \"#666666\"]\n",
    "    colors = [mplcolor.to_hex(c) for c in sns.color_palette(\"tab10\")]\n",
    "    remaining_colors = ['#d95f02', '#d95f02', '#d95f02', '#d95f02', '#d95f02', '#d95f02', '#7570b3', '#1f78b4', '#e31a1c']\n",
    "\n",
    "    #sns.set(rc={'figure.figsize':(20, 10)})\n",
    "    plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "    plt.rcParams['legend.fontsize'] = 14\n",
    "    \n",
    "    deltas_wiki_views = dict()\n",
    "    diff_median = dict()\n",
    "\n",
    "    for f in csv_files_preprocess:\n",
    "        city_wikipedia = f.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        city = f.split(\"\\\\\")[-1].split(\".\")[0].replace(\",\", \" \")  \n",
    "\n",
    "        colors_langs_dict = {\"English\": \"#666666\", \n",
    "                             \"Ukrainian\": \"#e7298a\",\n",
    "                             \"Russian\": \"#1b9e77\"}\n",
    "\n",
    "        df = pd.read_csv(f, low_memory=False)\n",
    "        \n",
    "        df.drop(columns = [\"Title\", \"Badges\"], inplace=True)\n",
    "        \n",
    "        df = df.merge(language_codes[[\"alpha2\", \"English\"]], how=\"left\", left_on=\"Language\", right_on=\"alpha2\").merge(language_codes[[\"alpha3-b\", \"English\"]], how=\"left\", left_on=[\"Language\", \"English\"], right_on=[\"alpha3-b\", \"English\"])\n",
    "        df[\"Wikipedia Page Language\"] = df[\"English\"]\n",
    "        df.drop(columns=\"English\", inplace=True)\n",
    "        df[\"Wikipedia Page Language\"].fillna(df[\"Language\"], inplace=True)\n",
    "        df.drop(columns=[\"Language\", \"alpha2\", \"alpha3-b\"], inplace=True)\n",
    "        df.set_index(\"Wikipedia Page Language\", inplace=True)\n",
    "        \n",
    "        official_langs = [\"Polish\"]\n",
    "        for i, l in enumerate(official_langs):\n",
    "            if l not in colors_langs_dict.keys():\n",
    "                colors_langs_dict[l] = remaining_colors[i]\n",
    "\n",
    "        languages = colors_langs_dict.keys() \n",
    "        languages = [l for l in languages if l in df.index]\n",
    "\n",
    "        df_weekly = df.T.reset_index()\n",
    "        df_weekly = df_weekly.assign(Weeks = df_weekly['index']).drop(columns = 'index')\n",
    "        df_weekly['Weeks'] =  df_weekly['Weeks'].astype('datetime64[ns]')\n",
    "        df_weekly = df_weekly.resample('W-Mon', label='left', closed = 'left', on='Weeks').sum()\n",
    "\n",
    "        df = df_weekly.T\n",
    "\n",
    "        df = df.loc[languages]\n",
    "\n",
    "        df_plot = df\n",
    "        df_plot.columns = df_plot.columns.astype('datetime64[ns]')\n",
    "        df_plot = df_plot.T\n",
    "\n",
    "        #Baseline + Pre-post war period\n",
    "        df_plot = df_plot[(df_plot.index >= \"2020-02-24\") & (df_plot.index <= \"2023-08-24\")]\n",
    "\n",
    "        #Assuming the period until 2018 as a baseline period, we calculate the relative change!\n",
    "        df_plot_baseline = df_plot[df_plot.index < \"2020-08-24\"]\n",
    "        dict_language_baseline = dict(df_plot_baseline.median())\n",
    "        print(\"MEDIAN VIEWS BASELINE 24.02-24.08.2020:\", dict_language_baseline)\n",
    "        \n",
    "        #in case baseline is 0 (no views at all)\n",
    "        for k,v in dict_language_baseline.items():\n",
    "            if v == 0:\n",
    "                dict_language_baseline[k] = 1\n",
    "\n",
    "        dict_language_median_pre = dict(df_plot[(df_plot.index >= \"2020-08-24\") & (df_plot.index < \"2022-02-24\")].median())\n",
    "        print(\"MEDIAN VIEWS PRE WAR 24.02.2022:       \", dict_language_median_pre)\n",
    "        dict_language_median_post = dict(df_plot[(df_plot.index > \"2022-02-24\")].median())\n",
    "        print(\"MEDIAN VIEWS POST WAR 24.02.2022:      \", dict_language_median_post)\n",
    "\n",
    "\n",
    "        df_plot_proportion = df_plot[df_plot.index >= \"2020-08-24\"]\n",
    "        df_plot_proportion = (df_plot_proportion-dict_language_baseline)/dict_language_baseline*100\n",
    "\n",
    "        axes = df_plot_proportion.plot.line(subplots=True, color=colors_langs_dict, lw=2)\n",
    "\n",
    "        axes = axes.flat  # .ravel() and .flatten() also work\n",
    "        # extract the figure object to use figure level methods\n",
    "        fig = axes[0].get_figure()\n",
    "\n",
    "        fig.suptitle(city_wikipedia + \" - Relative change in the number of views\\nBaseline period: 24.02.2020-24.08.2020\", fontsize=14)\n",
    "\n",
    "        # iterate through each axes to use axes level methods\n",
    "        for ax in axes:\n",
    "            ax.legend(handletextpad=-2.0, handlelength=0, loc='upper left', bbox_to_anchor=(0.08, 1.1), frameon=False)\n",
    "            ax.vlines(x=\"2022-02-24\", \n",
    "                   ymin=int(min(df_plot_proportion.min())), \n",
    "                   ymax=int(max(df_plot_proportion.max())), linestyles=\"dashed\", color=\"gray\")\n",
    "\n",
    "        plt.savefig(\"figs/series/relative-change-views-\" + city + \".png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        views_post_war = df_plot_proportion[df_plot_proportion.index > \"2022-02-24\"].sum()\n",
    "        views_pre_war = df_plot_proportion[df_plot_proportion.index < \"2022-02-24\"].sum()\n",
    "\n",
    "        deltas_wiki_views[city] = dict(views_post_war - views_pre_war)\n",
    "        diff_median[city] = dict()\n",
    "        for k, v in dict_language_median_post.items():\n",
    "            diff_median[city][k] = v - dict_language_median_pre.get(k, 0)\n",
    "\n",
    "    return deltas_wiki_views, diff_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_files_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deltas_wiki_views, diffs_median = timeseries_relative_change(csv_files_preprocess, same_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = False\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranking(ranking_1, ranking_2, language=\"Ukrainian\"):\n",
    "    \n",
    "    pallet1 = ['#8dd3c7', '#ffffb3', '#bebada', '#fb8072', '#80b1d3', '#fdb462', '#b3de69', '#fccde5', '#d9d9d9', '#bc80bd']\n",
    "    pallet2 = ['#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a']\n",
    "    pallet3 = ['#a50026', '#d73027', '#f46d43', '#fdae61', '#fee090', '#e0f3f8', '#abd9e9', '#74add1', '#4575b4', '#313695']\n",
    "    pallet4 = ['#6f0000', '#7f0000', '#8f0000', '#9f0000', '#af0000', '#bf0000', '#cf0000', '#df0000', '#ef0000', '#ff0000']\n",
    "    pallet_mpidr = ['#005869', '#ef7b00', '#8e203c', '#3e2c51', '#22b472', '#faae3b', '#0e7aac']\n",
    "    pallet_mpi_reds = ['#8e203c', '#c0272c', '#d8361a', '#ef7b00', '#faae3b']\n",
    "    greys = ['#808080']*10\n",
    "    \n",
    "    ranking_1_int = dict()\n",
    "    for i, location in enumerate(ranking_1):\n",
    "        ranking_1_int[location] = len(ranking_1) - i\n",
    "    \n",
    "    ranks = dict()\n",
    "    for location in ranking_1:\n",
    "        ranks[location] = [ranking_1_int[location]]\n",
    "\n",
    "    for i, location in enumerate(ranking_2):\n",
    "        ranks[location] += [len(ranking_2) - i] \n",
    "\n",
    "    tableau20 = pallet_mpi_reds + greys + greys \n",
    "    \n",
    "    # Avoid unnecessary whitespace.    \n",
    "    plt.ylim(0, 16)    \n",
    "    plt.xlim(1, 1.5)    \n",
    "\n",
    "    # Make sure your axis ticks are large enough to be easily read.    \n",
    "    # You don't want your viewers squinting to read your plot. \n",
    "    y_ticks_label, y_ticks = zip(*sorted(ranking_1_int.items(), key=lambda kv:kv[1]))\n",
    "    plt.yticks(y_ticks, y_ticks_label, fontsize=18)    \n",
    "    plt.xticks([1,1.5], [\"PESEL stocks\", \n",
    "                        \"Delta median relative change \\n \" + language + \" Wikipedia views\"], fontsize=18)    \n",
    "        \n",
    "    # Remove the tick marks; they are unnecessary with the tick lines we just plotted.    \n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",    \n",
    "                    labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")    \n",
    "    \n",
    "    locations = ranking_1\n",
    "\n",
    "    for i, location in enumerate(ranking_1):    \n",
    "        if i < 10: \n",
    "            plt.plot([1,1.5], ranks[location], lw=4, color=tableau20[i])    \n",
    "        else:\n",
    "            plt.plot([1,1.5], ranks[location], lw=2, color=tableau20[i]) \n",
    "\n",
    "        # Add a text label to the right end of every line. Most of the code below    \n",
    "        # is adding specific offsets y position because some labels overlapped.    \n",
    "        y_pos = ranks[location][1]\n",
    "\n",
    "        plt.text(1.55, y_pos, location, fontsize=18, color=tableau20[i])    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figs/ranks/rank_polish_cities_\" + language + \".png\", bbox_inches=\"tight\", format=\"png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in [\"Ukrainian\", \"Russian\", \"English\", \"Polish\"]:\n",
    "    rank_wiki_deltas = pd.DataFrame.from_dict(dict([(c[0], c[1][language]) \n",
    "                                                for c in sorted(deltas_wiki_views.items(), \n",
    "                                                                key=lambda x:x[1][language], reverse=True) \n",
    "                                                if c[0] != 'Poland']), orient=\"index\")\n",
    "    rank_wiki_deltas.columns = [\"Delta sum relative change\"]\n",
    "\n",
    "    rank_wiki_medians = pd.DataFrame.from_dict(dict([(c[0], c[1][language]) \n",
    "                                                for c in sorted(diffs_median.items(), \n",
    "                                                                key=lambda x:x[1][language], reverse=True) \n",
    "                                                if c[0] != 'Poland']), orient=\"index\")\n",
    "    rank_wiki_medians.columns = [\"Delta median relative change\"]\n",
    "    \n",
    "    ranks = rank_PESEL.merge(rank_wiki_deltas, right_index=True, left_index=True)\n",
    "    ranks = ranks.merge(rank_wiki_medians, right_index=True, left_index=True)\n",
    "    \n",
    "    print(language)\n",
    "    rho, p = spearmanr(ranks[\"Max PESEL stocks\"], ranks[\"Delta sum relative change\"])\n",
    "    print(\"Delta sum\", rho, p)\n",
    "    \n",
    "    rho, p = spearmanr(ranks[\"Max PESEL stocks\"], ranks[\"Delta median relative change\"])\n",
    "    print(\"Delta median\", rho, p)\n",
    "    \n",
    "    plot_ranking(ranks[\"Max PESEL stocks\"].sort_values(ascending=False).index.tolist(), \n",
    "                 ranks[\"Delta median relative change\"].sort_values(ascending=False).index.tolist(),\n",
    "                 language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.spines.left'] = True\n",
    "plt.rcParams['axes.spines.right'] = True\n",
    "plt.rcParams['axes.spines.top'] = True\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PESEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dane.drop(index=df_dane.index[0], axis=0, inplace=True)\n",
    "df_dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dane.columns = [c.capitalize() + \" DANE\" for c in df_dane.columns]\n",
    "df_dane[\"Date\"] = df_dane.index.astype('datetime64[ns]')\n",
    "df_dane.set_index(\"Date\", inplace=True, drop=True) \n",
    "df_dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_dane.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets: \n",
    "## Wikipedia views in Polish cities and Ukrainian refugees crossing the border to Poland "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_official\n",
    "\n",
    "for f in csv_files_preprocess:\n",
    "    city = f.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    \n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    df.drop(columns = [\"Title\", \"Badges\"], inplace=True)\n",
    "    df = df[df[\"Language\"].isin(colors_langs_dict.keys())]\n",
    "    df.set_index(\"Language\", inplace=True)\n",
    "\n",
    "    df.columns = df.columns.astype('datetime64[ns]')\n",
    "    df = df.T\n",
    "    \n",
    "    ## COMBINING DATASETS: UNHRC + WIKIPEDIA VIEWS (PL, EN, UK)\n",
    "    df_merged = df_merged.merge(df.add_prefix(city + \" \"), how=\"right\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_official = pd.DataFrame(df_merged[\"Ukrainian refugees\"].dropna())\n",
    "df_merged_official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "plt.plot(df_merged_official[\"Ukrainian refugees\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (30, 30)\n",
    "corr = df_merged.corr().round(2)\n",
    "sns.heatmap(corr, cmap=\"YlGnBu\", annot=True, vmin=0, vmax=1)\n",
    "plt.title(\"Correlation between number of views on Wikipedia page \\n dedicated to Polish cities across different languages and \\n the number of Ukrainian refugees crossing the border \\n from Ukraine to Poland during the war\")\n",
    "plt.savefig(\"figs/corr/correlations.png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPS = {\n",
    "    'boxprops':{'facecolor':'white', 'edgecolor':'black'},\n",
    "    'medianprops':{'color':'black'},\n",
    "    'whiskerprops':{'color':'black'},\n",
    "    'capprops':{'color':'black'}\n",
    "}\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n",
    "# Store the list of columns\n",
    "suffix_columns_to_plot = ['en', 'pl', 'ru', 'uk']\n",
    "\n",
    "# Create the figure and two subplots\n",
    "fig, axes = plt.subplots(ncols=len(suffix_columns_to_plot))\n",
    "\n",
    "# Create the boxplot with Seaborn\n",
    "for c, axis in zip(suffix_columns_to_plot, axes):\n",
    "           \n",
    "    df_aux = df_merged[[\"Ukrainian refugees\"] + [col for col in df_merged if c in col]]\n",
    "    df_aux = df_aux.dropna()\n",
    "    corrs = df_aux.corr().iloc[1:-1,0].tolist()\n",
    "    sns.boxplot(data=corrs, ax=axis, **PROPS) \n",
    "    if c == 'en':\n",
    "        #axis.set_title(\"Correlation between number of views on Wikipedia pages dedicated to \\nPolish cities across different languages and the number of Ukrainian \\nrefugees crossing the border from Ukraine to Poland (24.02.22-07.03.23)\", \n",
    "        #              loc=\"left\")\n",
    "        axis.set(ylabel=\"Correlation\", xlabel=\"English\", xticklabels=[\"English\"], xticks=[], ylim=(-0.3, 0.7))\n",
    "        \n",
    "    elif c == \"pl\":\n",
    "        axis.set(yticks=[], xlabel=\"Polish\", xticklabels=[\"Polish\"], xticks=[], ylim=(-0.3, 0.7))\n",
    "        \n",
    "    elif c == \"ru\":\n",
    "        axis.set(yticks=[], xlabel=\"Russian\", xticklabels=[\"Russian\"], xticks=[], ylim=(-0.3, 0.7))\n",
    "        \n",
    "    elif c == \"uk\":\n",
    "        axis.set(yticks=[], xlabel=\"Ukrainian\", xticklabels=[\"Ukrainian\"], xticks=[], ylim=(-0.3, 0.7))\n",
    "        \n",
    "    axis.axhline(0, ls='--', c='gray')\n",
    "    \n",
    "\n",
    "plt.savefig(\"figs/corr/corr-boxplot.png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[[\"Ukrainian refugees\"] + [col for col in df_merged if \"en\" in col]].dropna().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[[\"Ukrainian refugees\"] + [col for col in df_merged if \"en\" in col]].dropna().corr().iloc[1:-1,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in csv_files_preprocess:\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    \n",
    "    city = f.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    \n",
    "    filter_col = [\"Ukrainian refugees\"] + [col for col in df_merged if col.startswith(city)]\n",
    "    \n",
    "    df = df_merged[filter_col]\n",
    "    df.columns = [c.split(\" \")[1] if city in c else c for c in df.columns]\n",
    "\n",
    "    df_plot = df_merged[filter_col]\n",
    "    df_plot.columns = [c.split(\" \")[1] if city in c else c for c in df.columns]\n",
    "    \n",
    "    ## DF PLOT!!! RANGE\n",
    "    df_plot = df_plot[\"2022-01-01\":]\n",
    "    \n",
    "    # LOG ALL VALUES or NORM MAX \n",
    "    #df_plot = (np.log(df_plot)).replace(-np.inf, 0)\n",
    "    #df_plot = df_plot / df_plot.max()\n",
    "    #df_plot = df_plot / df_plot.sum()\n",
    "      \n",
    "    \n",
    "    fig, axes1 = plt.subplots(4, 1)\n",
    "    \n",
    "    df_plot[[\"Ukrainian refugees\"]].plot.line(\n",
    "        ax=axes1[0], color=colors_langs_dict, title=\"Wikipedia page: \" + city, ylim=(0, 150000), legend=None)\n",
    "    df_plot[[\"Ukrainian refugees\"]].plot.line(\n",
    "        ax=axes1[1], color=colors_langs_dict, ylim=(0, 150000), legend=None)\n",
    "    df_plot[[\"Ukrainian refugees\"]].plot.line(\n",
    "        ax=axes1[2], color=colors_langs_dict, ylim=(0, 150000), legend=None)\n",
    "    df_plot[[\"Ukrainian refugees\"]].plot.line(\n",
    "        ax=axes1[3], color=colors_langs_dict, ylim=(0, 150000), legend=None)\n",
    "    \n",
    "    axes1[3].set_xlabel('Time (days)')\n",
    "    axes1[1].set_ylabel('Number of Ukrainian refugees (source: UNHCR)', color=colors_langs_dict[\"Ukrainian refugees\"])\n",
    "    \n",
    "    axes20 = axes1[0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    axes21 = axes1[1].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    axes22 = axes1[2].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    axes23 = axes1[3].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    \n",
    "    df_plot[[\"pl\"]].plot.line(\n",
    "        ax=axes20, color=colors_langs_dict, xticks=[], ylim=(0, df_plot[\"pl\"].max()*1.1), legend=None)\n",
    "    axes20.set_ylabel('Number of views \\n (Polish)', color=colors_langs_dict[\"pl\"])\n",
    "    axes20.tick_params(axis='y', labelcolor=colors_langs_dict[\"pl\"])\n",
    "    \n",
    "    df_plot[[\"en\"]].plot.line(\n",
    "        ax=axes21, color=colors_langs_dict, xticks=[], ylim=(0, df_plot[\"en\"].max()*1.1), legend=None)\n",
    "    axes21.set_ylabel('Number of views \\n (English)', color=colors_langs_dict[\"en\"])\n",
    "    axes21.tick_params(axis='y', labelcolor=colors_langs_dict[\"en\"])\n",
    "    \n",
    "    df_plot[[\"ru\"]].plot.line(\n",
    "        ax=axes22, color=colors_langs_dict, xticks=[], ylim=(0, df_plot[\"ru\"].max()*1.1), legend=None)\n",
    "    axes22.set_ylabel('Number of views \\n (Russian)', color=colors_langs_dict[\"ru\"])\n",
    "    axes22.tick_params(axis='y', labelcolor=colors_langs_dict[\"ru\"])\n",
    "    \n",
    "    df_plot[[\"uk\"]].plot.line(\n",
    "        ax=axes23, color=colors_langs_dict, ylim=(0, df_plot[\"uk\"].max()*1.1), legend=None)\n",
    "    axes23.set_ylabel('Number of views \\n (Ukrainian)', color=colors_langs_dict[\"uk\"])\n",
    "    axes23.tick_params(axis='y', labelcolor=colors_langs_dict[\"uk\"])\n",
    "    \n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.savefig(\"figs/series/plot\" + city + \".png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (5, 4)\n",
    "    df_plot.rename(columns={\"Ukrainian refugees\": \"Ukrainian refugees \\n in Poland (UNHCR)\", \n",
    "                       \"pl\": \"Wikipedia views \\n in Polish\", \n",
    "                       \"en\": \"Wikipedia views \\n in English\", \n",
    "                       \"ru\": \"Wikipedia views \\n in Russian\", \n",
    "                       \"uk\": \"Wikipedia views \\n in Ukrainian\"}, inplace=True)\n",
    "    \n",
    "    corr = df_plot.dropna().corr()\n",
    "    sns.heatmap(corr, cmap=\"YlGnBu\", annot=True, vmin=0, vmax=1)\n",
    "    #plt.title(city + \": Correlation between number of views on Wikipedia page \\n dedicated to \" + city + \" across different languages and \\n the number of Ukrainian refugees crossing the border \\n from Ukraine to Poland (24.02.22-07.03.23)\")\n",
    "    plt.savefig(\"figs/corr/corr-\" + city + \".png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Null hypothesis for grangercausalitytests is that the time series in the second column, x2, does NOT Granger cause the time series in the first column, x1. Grange causality means that past values of x2 have a statistically significant effect on the current value of x1, taking past values of x1 into account as regressors. We reject the null hypothesis that x2 does not Granger cause x1 if the pvalues are below a desired size of the test.\n",
    "\n",
    "https://www.machinelearningplus.com/time-series/granger-causality-test-in-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Granger Causality test is used to determine whether or not one time series is useful for forecasting another.\n",
    "\n",
    "This test uses the following null and alternative hypotheses:\n",
    "\n",
    "Null Hypothesis (H0): Time series x does not Granger-cause time series y\n",
    "\n",
    "Alternative Hypothesis (HA): Time series x Granger-causes time series y\n",
    "\n",
    "The term “Granger-causes” means that knowing the value of time series x at a certain lag is useful for predicting the value of time series y at a later time period.\n",
    "\n",
    "This test produces an F test statistic with a corresponding p-value. If the p-value is less than a certain significance level (i.e. α = .05), then we can reject the null hypothesis and conclude that we have sufficient evidence to say that time series x Granger-causes time series y.\n",
    "\n",
    "https://www.statology.org/granger-causality-test-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in csv_files_preprocess:\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (5, 4)\n",
    "    \n",
    "    city = f.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    \n",
    "    filter_col = [\"Ukrainian refugees\"] + [col for col in df_merged if col.startswith(city)]\n",
    "    \n",
    "    df = df_merged[filter_col]\n",
    "    df.columns = [c.split(\" \")[1] if city in c else c for c in df.columns]\n",
    "   \n",
    "    ## WAR PERIOD\n",
    "    df = df[\"2022-02-24\":\"2023-03-07\"]    \n",
    "    \n",
    "    df.rename(columns={\"Ukrainian refugees\": \"Ukrainian refugees \\n in Poland (UNHCR)\", \n",
    "                       \"pl\": \"Wikipedia views \\n in Polish\", \n",
    "                       \"en\": \"Wikipedia views \\n in English\", \n",
    "                       \"ru\": \"Wikipedia views \\n in Russian\", \n",
    "                       \"uk\": \"Wikipedia views \\n in Ukrainian\"}, inplace=True)\n",
    "    \n",
    "    df_dot_f = df.T.dot(df)\n",
    "    labels = df.T.dot(df)\n",
    "    \n",
    "    for r in df_dot_f.index:\n",
    "        for c in df_dot_f.columns:\n",
    "            df_dot_f.loc[r,c] = np.nan\n",
    "            labels.loc[r,c] = \"\"\n",
    "            \n",
    "            if r != c:\n",
    "                g_causality = grangercausalitytests(df[[r,c]], maxlag=[3], verbose=False)[3][0][\"params_ftest\"]\n",
    "                \n",
    "                fscore = g_causality[0]\n",
    "                pvalue = g_causality[1]\n",
    "                \n",
    "                df_dot_f.loc[r,c] = fscore\n",
    "                labels.loc[r,c] = str(round(fscore,2)) + \"\\n(\" + str(round(pvalue, 2)) + \")\" # 0 -> F, 1 -> p-value\n",
    "                \n",
    "\n",
    "    sns.heatmap(df_dot_f, cmap=\"YlGnBu\", annot=labels, fmt = '', vmin=0, vmax=max(df_dot_f.max())+1)\n",
    "    #plt.title(city + \": F-test for Granger causality (column Granger cause row)\\n(24.02.22-07.03.23)\")\n",
    "    plt.savefig(\"figs/granger/granger-f-\" + city + \".png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
